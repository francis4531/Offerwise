"""
Analysis Caching System - Ensures Deterministic Results
Deploy this immediately for consistency
"""

import hashlib
import json
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
import sqlite3
import os

# ANALYSIS VERSION: Increment this when Risk DNA calculation changes
# This invalidates old cache entries automatically
ANALYSIS_VERSION = "5.0.0"  # v5.59.8: All AI calls now temperature=0 for determinism

class AnalysisCache:
    """
    Cache analysis results for identical inputs
    
    Benefits:
    - Same property = EXACT same results (100% deterministic)
    - 20x faster (instant response from cache)
    - Reduces API costs
    - Legal audit trail
    """
    
    def __init__(self, db_path: str = 'analysis_cache.db'):
        """Initialize cache with SQLite database"""
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self):
        """Create cache table if not exists"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_cache (
                cache_key TEXT PRIMARY KEY,
                property_address TEXT,
                asking_price INTEGER,
                result TEXT,
                created_at TIMESTAMP,
                accessed_at TIMESTAMP,
                access_count INTEGER DEFAULT 1
            )
        ''')
        
        # Index for cleanup queries
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_created_at 
            ON analysis_cache(created_at)
        ''')
        
        conn.commit()
        conn.close()
    
    def generate_cache_key(self, 
                          inspection_text: str, 
                          disclosure_text: str,
                          asking_price: int,
                          buyer_profile: Dict) -> str:
        """
        Generate deterministic cache key
        
        Same inputs + same analysis version = same key = same cached result
        When analysis logic changes, version changes = new cache entries
        """
        # Normalize texts (whitespace doesn't matter)
        inspection_norm = ' '.join(inspection_text.lower().split())
        disclosure_norm = ' '.join(disclosure_text.lower().split())
        
        # Normalize buyer profile (sort keys for consistency)
        profile_norm = json.dumps(buyer_profile, sort_keys=True)
        
        # Create content string with version
        content = f"{ANALYSIS_VERSION}|{inspection_norm}|{disclosure_norm}|{asking_price}|{profile_norm}"
        
        # SHA-256 hash (deterministic)
        cache_key = hashlib.sha256(content.encode()).hexdigest()
        
        logging.info(f"Generated cache key: {cache_key[:16]}... (v{ANALYSIS_VERSION}) for price ${asking_price:,}")
        
        return cache_key
    
    def get(self, cache_key: str) -> Optional[Dict[Any, Any]]:
        """
        Get cached analysis if exists and not expired
        
        Returns:
            Cached analysis dict or None
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            '''SELECT result, created_at, access_count 
               FROM analysis_cache 
               WHERE cache_key = ?''',
            (cache_key,)
        )
        
        row = cursor.fetchone()
        
        if row:
            result_json, created_at, access_count = row
            created_dt = datetime.fromisoformat(created_at)
            
            # Check if expired (90 days)
            if datetime.now() - created_dt < timedelta(days=90):
                # Update access tracking
                cursor.execute(
                    '''UPDATE analysis_cache 
                       SET accessed_at = ?, access_count = ?
                       WHERE cache_key = ?''',
                    (datetime.now().isoformat(), access_count + 1, cache_key)
                )
                conn.commit()
                conn.close()
                
                # Parse and return result
                result = json.loads(result_json)
                result['from_cache'] = True
                result['cache_hit'] = True
                
                logging.info(f"âœ… Cache HIT: {cache_key[:16]}... (accessed {access_count + 1} times)")
                
                return result
        
        conn.close()
        logging.info(f"âŒ Cache MISS: {cache_key[:16]}...")
        return None
    
    def set(self, cache_key: str, analysis: Dict[Any, Any], 
            property_address: str = "", asking_price: int = 0):
        """
        Store analysis in cache
        
        Args:
            cache_key: Cache key
            analysis: Analysis result dict
            property_address: Property address (for debugging)
            asking_price: Asking price (for debugging)
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Remove cache metadata before storing
        analysis_clean = analysis.copy()
        analysis_clean.pop('from_cache', None)
        analysis_clean.pop('cache_hit', None)
        
        cursor.execute(
            '''INSERT OR REPLACE INTO analysis_cache 
               (cache_key, property_address, asking_price, result, created_at, accessed_at, access_count)
               VALUES (?, ?, ?, ?, ?, ?, 1)''',
            (
                cache_key,
                property_address,
                asking_price,
                json.dumps(analysis_clean),
                datetime.now().isoformat(),
                datetime.now().isoformat()
            )
        )
        
        conn.commit()
        conn.close()
        
        logging.info(f"ðŸ’¾ Cached analysis: {cache_key[:16]}... for {property_address}")
    
    def cleanup_old_entries(self, days: int = 90):
        """Remove cache entries older than X days"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()
        
        cursor.execute(
            'DELETE FROM analysis_cache WHERE created_at < ?',
            (cutoff_date,)
        )
        
        deleted = cursor.rowcount
        conn.commit()
        conn.close()
        
        logging.info(f"ðŸ—‘ï¸ Cleaned up {deleted} old cache entries (older than {days} days)")
        
        return deleted
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Total entries
        cursor.execute('SELECT COUNT(*) FROM analysis_cache')
        total_entries = cursor.fetchone()[0]
        
        # Total accesses
        cursor.execute('SELECT SUM(access_count) FROM analysis_cache')
        total_accesses = cursor.fetchone()[0] or 0
        
        # Cache hit rate (accesses > 1 means cache was hit)
        cursor.execute('SELECT COUNT(*) FROM analysis_cache WHERE access_count > 1')
        cache_hits = cursor.fetchone()[0]
        
        # Average price
        cursor.execute('SELECT AVG(asking_price) FROM analysis_cache WHERE asking_price > 0')
        avg_price = cursor.fetchone()[0] or 0
        
        conn.close()
        
        cache_hit_rate = (cache_hits / total_entries * 100) if total_entries > 0 else 0
        
        return {
            'total_entries': total_entries,
            'total_accesses': total_accesses,
            'cache_hits': cache_hits,
            'cache_hit_rate': f"{cache_hit_rate:.1f}%",
            'avg_property_price': f"${avg_price:,.0f}"
        }


# Example usage in app.py
def analyze_property_with_cache(inspection_text, disclosure_text, 
                               asking_price, buyer_profile, property_address):
    """
    Main analysis function with caching
    
    This ensures EXACT same results for same inputs
    """
    
    # Initialize cache
    cache = AnalysisCache()
    
    # Generate cache key
    cache_key = cache.generate_cache_key(
        inspection_text=inspection_text,
        disclosure_text=disclosure_text,
        asking_price=asking_price,
        buyer_profile=buyer_profile
    )
    
    # Try to get from cache
    cached_result = cache.get(cache_key)
    if cached_result:
        logging.info(f"âœ… Returning cached analysis (instant response)")
        return cached_result
    
    # Not in cache - run full analysis
    logging.info(f"ðŸ”„ Running full analysis (first time for this property)")
    
    # Run your existing analysis
    intelligence = OfferWiseIntelligence()
    result = intelligence.analyze_property(
        inspection_text=inspection_text,
        disclosure_text=disclosure_text,
        property_price=asking_price,
        buyer_profile=buyer_profile
    )
    
    # Add metadata
    result['from_cache'] = False
    result['cache_key'] = cache_key
    
    # Cache the result for next time
    cache.set(
        cache_key=cache_key,
        analysis=result,
        property_address=property_address,
        asking_price=asking_price
    )
    
    return result


if __name__ == '__main__':
    # Test the cache
    cache = AnalysisCache()
    
    # Test data
    test_inspection = "Foundation shows cracks. Roof needs replacement. Electrical panel outdated."
    test_disclosure = "Seller aware of roof age. Foundation cracks disclosed."
    test_price = 950000
    test_profile = {'risk_tolerance': 'moderate', 'max_budget': 1000000}
    
    # First call - will compute
    print("First call (will compute)...")
    key = cache.generate_cache_key(test_inspection, test_disclosure, test_price, test_profile)
    result1 = cache.get(key)
    print(f"Result: {result1}")  # None
    
    # Simulate storing a result
    fake_result = {
        'risk_score': 72,
        'recommended_offer': 845000,
        'risk_tier': 'MODERATE'
    }
    cache.set(key, fake_result, "123 Main St", test_price)
    
    # Second call - will be cached
    print("\nSecond call (from cache)...")
    result2 = cache.get(key)
    print(f"Result: {result2}")
    print(f"From cache: {result2.get('from_cache')}")  # True
    
    # Stats
    print("\nCache stats:")
    stats = cache.get_stats()
    for k, v in stats.items():
        print(f"  {k}: {v}")
